{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import optuna\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import warnings\n",
    "project_path = Path.cwd().parent\n",
    "sys.path.append(project_path.as_posix())\n",
    "from pipeline.p0_data_loader import DataLoader\n",
    "from pipeline.p1_model_trainer import XGBoostTrainer\n",
    "from pipeline.p2_optuna_hpo import objective\n",
    "from pipeline.p3_model_evaluator import ModelEvaluator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/erdemkarakoylu/data/craig_pfc_2023/step_2_cleaned/df_phy.pqt'),\n",
       " PosixPath('/Users/erdemkarakoylu/data/craig_pfc_2023/step_2_cleaned/df_rrs.pqt'),\n",
       " PosixPath('/Users/erdemkarakoylu/data/craig_pfc_2023/step_2_cleaned/df_env.pqt'),\n",
       " PosixPath('/Users/erdemkarakoylu/data/craig_pfc_2023/step_2_cleaned/df_all.pqt')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path.home() / 'data/craig_pfc_2023/step_2_cleaned'\n",
    "assert data_path.exists()\n",
    "[i for i in data_path.glob('*.pqt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 15:00:21.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipeline.p0_data_loader\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m18\u001b[0m - \u001b[34m\u001b[1mData directory set to /Users/erdemkarakoylu/data/craig_pfc_2023/step_2_cleaned\u001b[0m\n",
      "\u001b[32m2025-03-17 15:00:21.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipeline.p0_data_loader\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[34m\u001b[1mRrs file used: df_rrs.pqt\u001b[0m\n",
      "\u001b[32m2025-03-17 15:00:21.052\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipeline.p0_data_loader\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[34m\u001b[1mPhytoplankton file use df_phy.pqt\u001b[0m\n",
      "\u001b[32m2025-03-17 15:00:21.052\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipeline.p0_data_loader\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m22\u001b[0m - \u001b[34m\u001b[1mEnv file used: df_env.pqt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(\n",
    "    data_path=data_path, rrs_file = 'df_rrs.pqt', \n",
    "    phy_file='df_phy.pqt', env_file='df_env.pqt')\n",
    "dX, dX_env, dY =  loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce features\n",
    "dX = dX.iloc[:, ::10]\n",
    "dX_env_sub = dX_env[['lat', 'temp']]\n",
    "dX = pd.concat((dX, dX_env_sub), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample data for quicker debugging iterations\n",
    "sample_size = 10000  # Use a smaller sample size for debugging\n",
    "dX = dX.sample(sample_size)\n",
    "dY = dY.loc[dX.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 15:00:24.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1m\n",
      "After subsampling: Features shape =(10000, 53),\n",
      "Targets shape =(10000, 7)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"\\nAfter subsampling: Features shape ={dX.shape},\\nTargets shape ={dY.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 15:00:24.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1m\n",
      "Train/Test split completed --> Train shape: (8000, 53), Test shape: (2000, 53)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Train/Test Split ---\n",
    "dX_train, dX_test, dY_train, dY_test = train_test_split(\n",
    "    dX, dY, test_size=0.2, random_state=42)\n",
    "logger.info(f\"\\nTrain/Test split completed --> Train shape: {dX_train.shape}, Test shape: {dX_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 15:00:26.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mInitial model trained with basic hyperparameters.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Train Initial Model & Evaluate ---\n",
    "# Define a basic set of hyperparameters for quick testing\n",
    "initial_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 3,\n",
    "    \"n_estimators\": 100,\n",
    "}\n",
    "model_trainer = XGBoostTrainer(initial_params)\n",
    "model = model_trainer.train_model(dX_train, dY_train)\n",
    "logger.info(\"Initial model trained with basic hyperparameters.\")\n",
    "\n",
    "# Run predictions on the test set\n",
    "preds = model.predict(dX_test)\n",
    "# Evaluate using the ModelEvaluator which computes MSE, RÂ², MAE, RMSE, etc.\n",
    "evaluator = ModelEvaluator()\n",
    "mse, r2, mae, rmse = evaluator.evaluate(dY_test, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 15:00:26.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mInitial Evaluation Results:\u001b[0m\n",
      "\u001b[32m2025-03-17 15:00:26.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mMSE: 0.002, R2: 0.757, MAE: 0.006, RMSE: 0.041\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Initial Evaluation Results:\")\n",
    "logger.info(f\"MSE: {mse:.3f}, R2: {r2:.3f}, MAE: {mae:.3f}, RMSE: {rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-17 15:00:26,157] A new study created in memory with name: no-name-66811ffa-cf72-4710-af0e-7b9e02885f30\n",
      "[I 2025-03-17 15:00:43,198] Trial 0 finished with value: 0.11714991469441438 and parameters: {'learning_rate': 0.003335253605086211, 'max_depth': 10, 'n_estimators': 237, 'subsample': 0.9663562244152417, 'colsample_bytree': 0.8693153033728531, 'gamma': 0.047656602310498085}. Best is trial 0 with value: 0.11714991469441438.\n",
      "[I 2025-03-17 15:01:04,465] Trial 1 finished with value: 0.16739570240757748 and parameters: {'learning_rate': 0.0011373693253201329, 'max_depth': 7, 'n_estimators': 293, 'subsample': 0.8033356217905692, 'colsample_bytree': 0.7285081766863049, 'gamma': 3.218280590206628e-05}. Best is trial 0 with value: 0.11714991469441438.\n",
      "[I 2025-03-17 15:01:12,335] Trial 2 finished with value: 0.03266205730100188 and parameters: {'learning_rate': 0.1775012165041938, 'max_depth': 3, 'n_estimators': 381, 'subsample': 0.5328384874271805, 'colsample_bytree': 0.6241765102097252, 'gamma': 6.530753120789088e-08}. Best is trial 2 with value: 0.03266205730100188.\n",
      "[I 2025-03-17 15:01:20,641] Trial 3 finished with value: 0.031904586782465105 and parameters: {'learning_rate': 0.12675939992184268, 'max_depth': 4, 'n_estimators': 294, 'subsample': 0.8036115207778418, 'colsample_bytree': 0.8510057935977824, 'gamma': 8.767235319443697e-06}. Best is trial 3 with value: 0.031904586782465105.\n",
      "[I 2025-03-17 15:01:26,455] Trial 4 finished with value: 0.033139124708612744 and parameters: {'learning_rate': 0.2050751863482197, 'max_depth': 6, 'n_estimators': 237, 'subsample': 0.5823614820285534, 'colsample_bytree': 0.7188692013202298, 'gamma': 0.0006304783011819936}. Best is trial 3 with value: 0.031904586782465105.\n",
      "\u001b[32m2025-03-17 15:01:26.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1m===  Basic Pipeline and Quick HPO Test Completed ===\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters from quick HPO test: {'learning_rate': 0.12675939992184268, 'max_depth': 4, 'n_estimators': 294, 'subsample': 0.8036115207778418, 'colsample_bytree': 0.8510057935977824, 'gamma': 8.767235319443697e-06}\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Quick Hyperparameter Optimization Test using Optuna ---\n",
    "# Run a small-scale hyperparameter optimization with 5 trials for debugging purposes.\n",
    "with mlflow.start_run():\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective(trial, dX_train, dY_train), n_trials=5)\n",
    "    best_params = study.best_trial.params\n",
    "    print(\"Best hyperparameters from quick HPO test:\", best_params)\n",
    "\n",
    "logger.info(\"===  Basic Pipeline and Quick HPO Test Completed ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Explanation and Interpretation\n",
    "\n",
    "1. **learning_rate**  \n",
    "   - **What it does:** Controls the step size at each boosting iteration. A smaller value means the model learns more slowly but can yield a more robust model if combined with a larger number of estimators.\n",
    "   - **Interpreting Values:**  \n",
    "     - **High Value (closer to 0.3):** Faster learning; risk of overshooting the optimal solution, potentially leading to overfitting.\n",
    "     - **Low Value (closer to 1e-3):** Slower learning; may require more estimators to converge, but can lead to better generalization.\n",
    "\n",
    "2. **max_depth**  \n",
    "   - **What it does:** Sets the maximum depth of each decision tree. This controls the complexity of the model.\n",
    "   - **Interpreting Values:**  \n",
    "     - **High Value (closer to 10):** Allows for deeper trees, capturing more complex patterns but with a higher risk of overfitting.\n",
    "     - **Low Value (closer to 3):** Results in shallower trees, which may underfit if the data is complex, but generally increases model generalizability.\n",
    "\n",
    "3. **n_estimators**  \n",
    "   - **What it does:** Specifies the number of boosting rounds (i.e., trees) to build.\n",
    "   - **Interpreting Values:**  \n",
    "     - **High Value:** More trees can lead to better performance on training data, but might also cause overfitting if not regulated by other parameters.\n",
    "     - **Low Value:** Fewer trees can lead to faster training and less overfitting, but might not capture enough complexity in the data.\n",
    "\n",
    "4. **subsample**  \n",
    "   - **What it does:** Represents the fraction of samples used for fitting each individual tree.\n",
    "   - **Interpreting Values:**  \n",
    "     - **High Value (closer to 1.0):** Uses most of the data for each tree, which can increase accuracy but may also increase overfitting.\n",
    "     - **Low Value (closer to 0.5):** Uses fewer samples per tree, introducing randomness that can reduce overfitting but might also lead to underfitting if too low.\n",
    "\n",
    "5. **colsample_bytree**  \n",
    "   - **What it does:** Specifies the fraction of features (columns) used when building each tree.\n",
    "   - **Interpreting Values:**  \n",
    "     - **High Value (closer to 1.0):** More features are used, which can increase accuracy but also the risk of overfitting.\n",
    "     - **Low Value (closer to 0.5):** Fewer features are used per tree, adding regularization and potentially improving generalizability.\n",
    "\n",
    "6. **gamma**  \n",
    "   - **What it does:** Sets the minimum loss reduction required to make a further partition on a leaf node. It acts as a regularization parameter.\n",
    "   - **Interpreting Values:**  \n",
    "     - **High Value:** Demands a larger reduction in loss to split a node, leading to simpler trees (more regularization). This can prevent overfitting.\n",
    "     - **Low Value:** Allows more splits even if the loss reduction is small, potentially capturing more complex patterns but increasing the risk of overfitting.\n",
    "\n",
    "### How to Use These Interpretations\n",
    "\n",
    "- **Optimized Values Context:**  \n",
    "  After running your hyperparameter optimization, review the best parameters:\n",
    "  - For example, if the optimized `learning_rate` is very low, it suggests that the model benefits from gradual learning, potentially indicating that the data is noisy or complex.\n",
    "  - A high `max_depth` might mean your data has complex interactions, but you should verify that the performance on the validation set is not a result of overfitting.\n",
    "  - Lower values for `subsample` or `colsample_bytree` indicate that some regularization was beneficial to avoid overfitting, especially given the high dimensionality (500+ features) of your data.\n",
    "  - A moderate to high `gamma` value might suggest that the model benefits from stronger regularization to avoid unnecessary splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptoa_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
