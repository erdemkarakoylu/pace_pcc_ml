{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170c66de",
   "metadata": {},
   "source": [
    "# Phytoplankton Primary Composition end-to-end: HPO → train → evaluate → SHAP (dataset-agnostic)\n",
    "\n",
    "This notebook runs the full pipeline using the installed `phyx` package.\n",
    "\n",
    "**Prereqs**\n",
    "- Environment created/activated\n",
    "- Package installed: `pip install -e . --no-deps`\n",
    "- Dataset folder with Parquet files:\n",
    "    * df_rrs.pqt # feature columns (e.g., Rrs_* and/or ancillary features)\n",
    "    * df_phyto.pqt # targets: dia, chl, cya, coc, din, pha, tot_chla\n",
    "    * df_env.pqt # optional (aligned rows)\n",
    "\n",
    "\n",
    "**Flow**\n",
    "1. Load features/targets (and optional env)\n",
    "2. Train/test split\n",
    "3. Optuna HPO (KFold inside objective)\n",
    "4. Train on full training split with best params\n",
    "5. Evaluate on test split; save metrics\n",
    "6. SHAP values + per-target summary plots\n",
    "7. (Optional) Train final production model on all data\n",
    "\n",
    "\n",
    "---\n",
    "## 1) Setup: imports, logging, paths, and versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43210469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erdemkarakoylu/miniconda3/envs/phyx/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-02 21:41:09.075 | INFO     | __main__:<module>:23 - Versions → numpy=2.2.6, pandas=2.2.3, xgboost=2.1.4, sklearn=1.6.1, shap=0.46.0, optuna=4.5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "# Configure concise logging for notebooks\n",
    "logger.remove()\n",
    "logger.add(lambda msg: print(msg, end=\"\"), level=\"INFO\")\n",
    "\n",
    "# --- Set your paths & options here ---\n",
    "#DATA_DIR = Path(\"your_data_path\")       # <-- change to your dataset folder\n",
    "DATA_DIR = Path.home() / 'data/craig_pfc_2023/step_2_cleaned'\n",
    "assert DATA_DIR.exists(), f\"Data directory {DATA_DIR} does not exist!\"\n",
    "OUTDIR   = Path(\"artifacts/pace_run\")     # where outputs are written\n",
    "N_TRIALS = 30                             # use 5–10 for a quick smoke test\n",
    "TEST_SIZE = 0.20\n",
    "SEED = 42\n",
    "\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Versions (optional but helpful)\n",
    "import numpy, pandas, xgboost, sklearn, shap, optuna\n",
    "logger.info(\n",
    "    \"Versions → numpy={}, pandas={}, xgboost={}, sklearn={}, shap={}, optuna={}\\n\",\n",
    "    numpy.__version__, pandas.__version__, xgboost.__version__,\n",
    "    sklearn.__version__, shap.__version__, optuna.__version__\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d2d9a4",
   "metadata": {},
   "source": [
    "## 2) Load data\n",
    "\n",
    "Loads `df_rrs.pqt` (features) and `df_phyto.pqt` (targets). If `df_env.pqt` exists, it’s concatenated to features. Basic sanity checks ensure numeric columns and aligned rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "043d35d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-02 21:46:41.145 | INFO     | phyx.pipeline.data_loader:load_data:27 - Loaded X=%s, Y=%s%s\n",
      "2025-09-02 21:46:41.231 | INFO     | __main__:<module>:17 - Loaded X: 1261607 rows × 44 cols | Y: 1261607 rows × 7 targets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phyx.pipeline.data_loader import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "dl = DataLoader(\n",
    "    DATA_DIR, rrs_file='df_rrs_pace_sub.pqt', env_file='df_env.pqt',\n",
    "    phy_file='df_phy.pqt')\n",
    "X_df, Xenv_df, Y_df = dl.load_data()\n",
    "if Xenv_df is not None:\n",
    "    X_df = X_df.join(Xenv_df)\n",
    "\n",
    "# Sanity checks\n",
    "non_numeric = [c for c in X_df.columns if not pd.api.types.is_numeric_dtype(X_df[c])]\n",
    "if non_numeric:\n",
    "    raise TypeError(f\"Non-numeric feature columns detected (first few): {non_numeric[:5]}\")\n",
    "assert X_df.shape[0] == Y_df.shape[0], \"Row count mismatch between X and Y.\"\n",
    "\n",
    "logger.info(\"Loaded X: {} rows × {} cols | Y: {} rows × {} targets\\n\",\n",
    "            X_df.shape[0], X_df.shape[1], Y_df.shape[0], Y_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6651f5",
   "metadata": {},
   "source": [
    "## 3) Train/test split + schema guard\n",
    "\n",
    "Create a deterministic split and verify that feature **names and order** match between train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d5f96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-02 21:53:54.008 | INFO     | __main__:<module>:4 - Split → train: 1009285 rows, test: 252322 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr, Xte, Ytr, Yte = train_test_split(X_df, Y_df, test_size=TEST_SIZE, random_state=SEED)\n",
    "logger.info(\"Split → train: {} rows, test: {} rows\\n\", Xtr.shape[0], Xte.shape[0])\n",
    "\n",
    "# Fail fast if schema differs\n",
    "if list(Xtr.columns) != list(Xte.columns):\n",
    "    missing = [c for c in Xtr.columns if c not in Xte.columns]\n",
    "    extra   = [c for c in Xte.columns if c not in Xtr.columns]\n",
    "    moved   = [c for c in Xtr.columns if c in Xte.columns and Xtr.columns.get_loc(c) != Xte.columns.get_loc(c)]\n",
    "    raise ValueError(f\"Feature schema mismatch.\\nMissing in test: {missing}\\nExtra in test: {extra}\\nReordered: {moved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89371af",
   "metadata": {},
   "source": [
    "## 4) Hyperparameter optimization (Optuna)\n",
    "\n",
    "Runs KFold CV inside the objective for each trial. Saves `best_params.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from phyx.pipeline.optuna_hpo import objective\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lambda trial: objective(trial, Xtr, Ytr), n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "best_params = {**study.best_trial.params, \"objective\": \"reg:squarederror\"}\n",
    "best_path = OUTDIR / \"best_params.json\"\n",
    "best_path.write_text(json.dumps(best_params, indent=2))\n",
    "\n",
    "logger.info(\"Best RMSE={:.4f}\\n\", study.best_value)\n",
    "logger.info(\"Saved best params → {}\\n\", best_path)\n",
    "best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee436e7",
   "metadata": {},
   "source": [
    "## 5) Train on full training split with best params; save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037bb49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phyx.pipeline.model_trainer import XGBoostTrainer\n",
    "\n",
    "trainer = XGBoostTrainer(params=best_params)\n",
    "trainer.fit_full(Xtr, Ytr)                    # remembers feature schema if DataFrames were used\n",
    "MODEL_PATH = OUTDIR / \"model.pkl\"\n",
    "trainer.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98575536",
   "metadata": {},
   "source": [
    "## 6) Evaluate on test split; save metrics\n",
    "\n",
    "Computes MSE, RMSE, MAE, and R² on the held-out test set and writes `metrics.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a929634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phyx.pipeline.model_evaluator import ModelEvaluator\n",
    "\n",
    "evaluator = ModelEvaluator()\n",
    "yhat = trainer.predict(Xte)   # will validate schema if DataFrame\n",
    "mse, r2, mae, rmse = evaluator.evaluate(Yte.to_numpy(), yhat)\n",
    "\n",
    "metrics = {\"mse\": float(mse), \"rmse\": float(rmse), \"mae\": float(mae), \"r2\": float(r2)}\n",
    "(OUTDIR / \"metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
    "logger.info(\"Metrics → {}\\n\", metrics)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593c3b8",
   "metadata": {},
   "source": [
    "## 7) SHAP on test split: values + per-target summary plots\n",
    "\n",
    "Computes SHAP values on a sample of the test set and saves summary plots (`shap_summary_target_*.png`). Displays the first few inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phyx.explain.shap_runner import run_shap_and_plots\n",
    "from IPython.display import Image, display\n",
    "import glob\n",
    "\n",
    "npz_path = run_shap_and_plots(MODEL_PATH, Xte, list(Y_df.columns), OUTDIR, nsamples=2000)\n",
    "plot_paths = sorted(glob.glob(str(OUTDIR / \"shap_summary_target_*.png\")))\n",
    "logger.info(\"Saved {} SHAP summary plots → {}\\n\", len(plot_paths), OUTDIR)\n",
    "\n",
    "# Preview up to 3 plots\n",
    "for p in plot_paths[:3]:\n",
    "    display(Image(filename=p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adcf5a1",
   "metadata": {},
   "source": [
    "## 8) (Optional) Train a final “production” model on **all data**\n",
    "\n",
    "This refits using the tuned hyperparameters on `(train + test)`. It **does not** overwrite the evaluated model; it saves `model_production.pkl`. DO NOT report metrics for this model (test was used for training) unless new data becomes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252644ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT_ALL = True  # set False to skip\n",
    "\n",
    "if FIT_ALL:\n",
    "    trainer.fit_full(X_df, Y_df)\n",
    "    PROD_MODEL = OUTDIR / \"model_production.pkl\"\n",
    "    trainer.save(PROD_MODEL)\n",
    "    logger.info(\"Saved production model trained on ALL data → {}\\n\", PROD_MODEL)\n",
    "else:\n",
    "    logger.info(\"Skipped production fit (FIT_ALL=False)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1341569d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phyx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
